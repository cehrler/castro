\subsubsection{Evaluation of the Query System}
\label{sec:evaluation_of_the_query_system}
The \textsc{History Explorer} is a quite complex retrieval and recommendation system and the proper
evaluation of its performance is for various reasons beyond the scope of this project. First, the
final systems offers a huge amount of combinations of different scoring schemas (e.g. TF-IDF,
cosine, NE-similarity), several differnt options for constraining queries through metadata (e.g.\
\texttt{date} or \texttt{type}) and upon this also several filtering, highlighting and clustering
algorithms. The complexity of these options makes it impossible to evaluate each combinations.

However, the most important problem that anticipates evaluation of the system is due to the
structure of the task and the dataset. The CSDB does not provide any kind of labels or tags on the
documents and thus quantitative evaluation is virtually impossible without manually tagging (all)
the documents by a domain expert (which again is virtually impossible). A task which is of course
too time consuming and expensive.

In a similar fashion, qualitative evaluation is doomed because evaluation being done by a group of
four (non-domain expert) people does not give any feasible statistical results. Furthermore, since
there is no ground truth on the data, in any case only precision scores but no recall can be
obtained.

In summary, the only proper way for evaluation of \textsc{History Explorer} is a user study with a
(statistically) sufficiently large group of domain experts (i.e.\ historians). The remainder of
this section will discuss common (subjectiv) observations that have been made by the group members
during the usage of \textsc{History Explorer}.

\subsubsection{Common Observations}
\label{sec:evaluation_common_observation}
\ignore{Ongoing experimentation of the group members with the recommendation system gave rise to
some common observations. First, the quality of the retrieved results is highly dependent on the
search query. Queries on political, economical and social topics will often result in good
recommendations which gives evidence that the system does work as desired.

For example a query for ``\texttt{Missile Crisis}'' reveals several related documents from the
timeperiod in question which cover the topic. Although many of the documents have 

What we could found out
 * Success/Failure of the recommendation system is highly dependent on the search query.
 * For political/economical and related topics the systems seems (lets say there is evidence) to
retrieve good results
 * Often fails for strange queries e.g. ``marriage father mother'' or even ``marriage''
 * normalization of term frequencies often gives bad results
    => for the query term RAILWAY the most relevant result was unrelated and contained the keyword
only once (mentioning the ...railway, health and tourism industry...)
 * search term ``cuban missile crisis'' does not wortk because cuban is so common and dominates the
results => ``missile crisis'' as query performs better.
    => moste relevant results and its neighbors talk about missile crisis, and are from the same
period of time}