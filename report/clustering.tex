For a recommendation system based on graphs, it may be a desirable feature to cluster the graph into
dense regions: It may be assumed that such regions correspond to highly similar or otherwise-related
documents which then form part of a larger related unit. Hence, being able to
automatically identify clusters can help to further discern between important and less important
documents.

Several algorithms for clustering of graphical data exist, including rather simple
hierarchical clustering methods and classical methods like $k$-means clustering
\cite{eosl2}. Algorithms which are currently popular are spectral clustering
methods, which are based on eigendecompositions of graph Laplacians \cite{spectral_clustering}.

Two different clustering methods were experimented with in this project: $k$-means clustering
as well as Chinese Whispering clustering, which was developed with NLP tasks in mind and has the
favorable property of automatically determining the number of clusters, while in $k$-means clustering this must be specified beforehand \cite{cw_clustering}.

\subsubsection{$k$-means Clustering}
\label{sec:k-means_clustering}
Firstly, $k$-means clustering was implemented and evaluated, where the VMS representation of
documents was used, i.e.\ the same vectors as used in the index representation of the documents (see
section \ref{sec:indexing}). However, one drawback of $k$-means is that the user needs to specify
the number of clusters $k$ beforehand.

The objective of $k$-means clustering is to minimize total intra-cluster variance. For the
initialization, $k$ cluster prototypes are randomly placed in the data space. The following two
steps are then iterated until convergence:
\begin{itemize}
  \item Assign each datapoint to its closest prototype.
  \item For each prototype, compute the center of mass over all assigned datapoints and place the
prototype in the new center of mass.
\end{itemize}

This approach did not perform very well: The resulting clusters were not the densely connected
regions, likely caused by the high dimensionality of the vector space used
and the sparseness of vectors representing the documents.

\subsubsection{Chinese Whispering Clustering}
\label{sec:chinese_whipsering_clustering}
As stated above, the Chinese Whispering clustering (CWC) algorithm was is particularly well-suited to NLP tasks. It is a
randomized time-linear graph clustering algorithm, with the special characteristic that it does not depend
on external parameters but rather adapts to the domain automatically \cite{cw_clustering}.

During initialization, each node is assigned to its own cluster. Then, in a randomized
fashion, each node is assigned to the currently highest ranked class in the local neighborhood. This is performed until the assignment of nodes to clusters ceases to change in the next update round. After few iterations the stable clustering is achieved (i.e.\ there are no changes in the cluster
assignments), with at most a few nodes, at the border of a cluster or clusters, which periodically flip their
cluster assignment.

Although the algorithm is rather simple, it performed well with the data in this project in comparison to $k$-means clustering, which did not work with such a high-dimensional space. CWC was productive in clustering documents in the given dataset, while $k$-means clustering was not. However, one of the drawbacks of CWC is that nodes are always assigned to the locally dominant cluster, even if the node should rather stay in its own cluster. Thus, when using CWC, it is possible that convergence is never achieved.

A variations of this algorithm was developed in this project to address this problem.
In the variation of the algorithm the cluster assignment sum needs to be greater than a predefined threshold in order to be assigned to a cluster different from its currently-assigned one. This threshold was set to the average edge strength of the entire graph multiplied by a constant able to be specified by the user.

Before the updating rounds begin, a predefined number of initialization steps are taken: All edges of the graph are sorted in the descending order according to
their weights. In each initialization step, one node is pulled out from the list and the standard
node cluster assignment procedure is then executed on the node for which the cluster assignment sum
is greater. This means that the cluster assignment sum need not exceed the precomputed threshold.

This pre-clustering is done in order to allow clustering to begin in spite of this threshold:
Initially, each node forms its own cluster, and many nodes may not exceed this threshold. Likewise,
selecting the nodes attached to the edges with great weight ensures that the cluster assignments
done during these initialization steps are not relevant for the final result.

\ignore{
The second variation has a working name "Give it to the poor CHWS". It differs from the first modification in that aspect, that there are no initialization steps and the weight of each cluster is normalized by dividing it by the term, whose value increases proportionality to the cluster size. In other words, this variation tends to produce smaller classes. We assume that this normalization is relevant, because in the experiments it happened a lot of times, that there was a cluster that become so big that it started to attract even the nodes that didn't have any strong connections to the nodes in the cluster - the document became the member of the cluster just because of a huge number of very weak almost irrelevant connections.}

For all the methods, if the number of clusters created exceeds the predefined maximum number of clusters, only the clusters with the greatest number of nodes are marked. The nodes that lay inside the other clusters are marked as not being included in any cluster.

Graph clustering has the potential to improve the recommendation functions of the application, helping the user to identify tightly-coupled sets of documents in larger graphs, which can be subjects of even further investigation. Both the original CWC algorithm and this modified, more conservative variant were included in the final application, with the ability to use either the original algorithm, the modified algorithm, or no clustering at all. This was done to facilitate the evaluation of the modified CWC algorithm over the original, as well as the evaluation of clustered graphs over non-clustered graphs. In fact, it is possible that each option has its advantages and disadvantages, and including each option in the application gives the user greater power in searching.
