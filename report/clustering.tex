For a recommendation system based on graphs, it may be a desirable feature to cluster the graph into
dense regions: It may be assumed that such regions correspond to highly similar or otherwise-related
documents which nevertheless form part of a larger related unit. Hence, being able to
automatically identify clusters can help to further discern between important and less important
documents.

Several algorithms for clustering of graphical data exist, including rather simple
hierarchical clustering methods and classical methods like $k$-means clustering
\cite{eosl2}. Two algorithms which are currently popular are spectral clustering
methods, which are based on eigendecompositions of graph Laplacians \cite{spectral_clustering}.

Two different clustering methods were experimented with in this project: $k$-means clustering
as well as Chinese Whispering clustering, which was developed with NLP tasks in mind and has the
favorable property of automatically determining the number of clusters, while in $k$-means clustering this must be specified beforehand \cite{cw_clustering}.

\subsubsection{$k$-means Clustering}
\label{sec:k-means_clustering}
Firstly, $k$-means clustering was implemented and evaluated, where the VMS representation of
documents was used, i.e.\ the same vectors as used in the index representation of the documents (see
section \ref{sec:indexing}). However, one drawback of $k$-means is that the user needs to specify
the number of clusters $k$ beforehand.

The purpose of $k$-means clustering is to minimize total intra-cluster variance. For the
initialization, $k$ cluster prototypes are randomly placed in the data space. The following two
steps are then iterated until convergence:
\begin{itemize}
  \item Assign each datapoint to its closest prototype.
  \item For each prototype, compute the center of mass over all assigned datapoints and place the
prototype in the new center of mass.
\end{itemize}

This approach did not perform very well: The resulting clusters were not the densely connected
regions, likely caused by the high dimensionality of the vector space used
and the sparseness of vectors representing the documents.

\subsubsection{Chinese Whispering Clustering}
\label{sec:chinese_whipsering_clustering}
As stated above, the Chinese Whispering clustering (CWC) algorithm was is particularly well-suited to NLP tasks. It is a
randomized time-linear graph clustering algorithm, with the special characteristic that it does not depend
on external parameters but rather adapts to the domain automatically \cite{cw_clustering}.

During initialization, each node is assigned to its own cluster. Then, in a randomized
fashion, each node is assigned to the currently highest ranked class in the local neighborhood. This is performed until the assignment of nodes to clusters ceases to change in the next update round. After few iterations the stable clustering is achieved (i.e.\ there are no changes in the cluster
assignments), with at most a few nodes, at the border of a cluster or clusters, which periodically flip their
cluster assignment.

Although the algorithm is rather simple, it performed well with the data in this project in comparison to $k$-means clustering, which did not work with such a high-dimensional space. CWC was productive in clustering documents in the given dataset, while $k$-means clustering was not. However, one of the drawbacks of CWC is that nodes are always assigned to the locally dominant cluster, even if the node should rather stay in its own cluster. Thus, when using CWC, it is possible that convergence is never achieved.

A variation of this algorithm was developed in this project to address this problem, where the cluster assignment sum needs to be greater than a predefined threshold in order to be assigned to a cluster different from its currently-assigned one. This threshold was set to
the average edge strength of the entire graph multiplied by a constant able to be specified by the user.

Before the updating rounds begin, a predefined number of initialization steps are taken: All edges of the graph are sorted in the descending order according to
their weights. In each initialization step, one node is pulled out from the list and the standard
node cluster assignment procedure is then executed on the node for which the cluster assignment sum
is greater. This means that the cluster assignment sum need not exceed the precomputed threshold.

By this we want to start up the clustering process because it may be hard to exceed the precomputed threshold at any node when all the nodes have their own cluster. Selecting the nodes attached to the edges with great weight ensures that the cluster assignments done during these initialization steps are not relevant.

The modified CWC algorithm is a recent feature and it was not properly evaluated yet.
Visual inspections with reasonable parameter settings revealed, that the the resulting cluster
assignment look better, compared to the ones obtained from classic CWC clustering. However, for the
clustering algorithm there is plenty of room for further improvement and a need for extended
evaluation. Ideas for improvement are e.g.\ experimentation with the thresholding constant
and implementation of different initialization schemas.

To summarize, graph clustering has the potential enhance our recommendation system in such a way
that it can help the user to identify tightly coupled sets of documents in larger graphs that
can be subject of further investigation.

