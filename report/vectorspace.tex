Vector Space Model (VSM) is a model for representing text document or any other data. In VSM for information retrieval both documents and queries are represented as vectors of numbers. The value of each element of the vector corresponds to the term importance for the representation of the query or the document. The main advantage of the VSM is that it has a convenient algebraic formulation. It is also more flexible than the boolean model. In boolean models documents that do not exactly match the query are rejected. However, in many real word scenarios the situation is different: the relevance of the document for the user is often graded rather then binary. The VSM can model this relevance scale.

The vector coefficients are usually normalized in order to have the same length, otherwise the model would favor the documents with greater length. What is important is the density of the term in the document.

\subsubsection{Indexing}
\label{sec:indexing}
Index is the data structure that stores the vector representation of the documents in the vector model. It is a matrix in which each row stores the vector representation of a single document. Each column stores the weights for one distinct term in all the documents. The index matrix is usually very sparse. In our implementation it is represented as a sparse matrix. In the memory we store only coordinatates of nonzero cells and their values. All the document representation vectors were normalized in advance. Which allows us to compute cosine similarities of documents and queries faster.

Our model contains several index matrices. We created an index matrix for each named entity type and a general index which indexes the non named entity terms. For all named entity types aliased or non-aliased index matrix can be used. Our current implementation of the VSM doesn't use TF-IDF measure for adjustment of query terms. We only use TF-IDF for adjusting the document representation. It means that all the values corresponding to the search terms in the query remain the same in the vector representation of the query. 


\subsubsection{Term Weighting}
\label{sec:term_weighting}
There are several measures that can be used to express the importance of the index term for the document. The most widely used are TF and TF-IDF. 
TF stands for term frequency, it measures directly how many times the word occurred in the document, TF score grows linearly with the number of occurrences of the term in the document. The TF score of term j in document i is defined as follows:

\[\text{TF}(i,j) = n(i,j) / \sum_{k \in \mathcal{T}}{n(i,k)}\]


$n(i,j)$ stands for the number of counts term $j$ occurred in document $i$.


TF-IDF stands for term frequency - inverse document frequency. This measure is motivated on the notion that the terms which occur in a lot of documents are not very useful for searching purposes. The terms which occur in a few documents only have greater discrimination power. The TF-IDF score of term $j$ in document is is defined as follows.

\[\text{TF-IDF}(i,j) = \text{TF}(i,j)\log{(|D| |\lbrace d : t(j) \in d \rbrace|)}\]

The TF-IDF score of term $j$ drops according to the logarithm of the number of documents that contain term $j$. The TF-IDF score of terms which are contained in every single document is always equal to zero.

\subsection{Computing Similarity between Documents}\label{sec:computing_similarity_between_documents}

In VSM we need to measure the similarity between vectors. The similarity can expressed by the cosine similarity measure:

\[\text{cosim}(q,d) = \sum_{k \in \mathcal{T}}\frac{q(k)d(k)}{|q||d|}\]

It expresses the cosine angle between the vectors q and d. This measure is equal to 1 for the paralell vectors and it's equal to zero for the perpendicular vectors.

We arrange the retrieved documents according to their similarity to each other. To this end, we store the similarity of each pair of documents in similarity matrices. The matrices correspond to the four different indexes: similarity according to persons, locations, organizations and general vocabulary. These matrices are not sparse and each cell is represented in the memory.   





\subsubsection{Keyword and Metadata Search}\label{sec:keyword_search}

In VSM the searching proceeds as follows. The search query is transformed into its vector representation in the same manner as it is done for the document. The cosine similarity between the query vector and the vector representation of each document is computed. The search results are ordered according to their similarity to the user query. 

Document retrieval in our model consists of two steps. In the first step, documents that do not satisfy the conditions concerning the type and date of the document, are filtered out. In the second step, documents are sorted in descending order according to their cosine measure similarity with the query. If the search is only on meta data, documents are not sorted. Only the subset of the best results of the size specified by the user proceeds to the next stage.



