The Vector Space Model (VSM) is a model for representing text documents or in general arbitrary data as vectors. In information retrieval, the VSM is often used to represent both documents and queries as points in the euclidean space. In the VSM the value of each element of the vector corresponds to the importance of a term for the representation of the query or the document. The main advantage of the VSM is that it has a convenient algebraic formulation. It is also more flexible than boolean models. In boolean models documents that do not exactly match the query are rejected. However, in many real word scenarios the situation is different: The relevance of the document for the user is often graded rather then binary. The VSM can model this relevance scale.

The vectors in VSM are usually normalized in order to have the same length (e.g.\ unit length), otherwise the model would favor larger documents.

\subsubsection{Indexing}
\label{sec:indexing}
Index is the data structure that stores the vector representation of the documents in a vector model. It is a matrix in which each row stores the vector representation of a single document. Each column stores the weights for one distinct term in all the documents. The index matrix is usually very sparse. In our implementation it is represented as a sparse matrix. In the memory we store only coordinates of nonzero cells and their values. All the document representation vectors were normalized in advance. Which allows us to compute cosine similarities of documents and queries faster.

Our model contains several index matrices. We created an index matrix for each NE type and a general index which indexes the non named entity terms. For all NE types, either aliased or non-aliased, index matrices can be used. Our current implementation of the VSM does not use a TF-IDF measure for the adjustment of query terms. We only use TF-IDF for adjusting to the document representation. This means that all the values corresponding to the search terms in the query remain the same in the vector representation of the query.

\subsubsection{Term Weighting}
\label{sec:term_weighting}
There are several measures that can be used to express the importance of the index term for the document. The most widely used are TF and TF-IDF.

TF stands for term frequency and it measures directly how many times a word occurred in the document. Hence, TF scores grow linearly with the number of occurrences of a term in the document. The TF score of term $i$ in document $d$ is defined as follows:
\[\text{TF}_d(i) = \frac{n_d(i)}{\sum_{i' \in \mathcal{T}}{n_d(i')}}\]
where $n_d(i)$ counts the number of occurences of term $i$ in document $d$ and $\mathcal{T}$ is the set of all terms. 

TF-IDF stands for term frequency - inverse document frequency. This measure is motivated on the notion that terms which occur in almost every document are not very useful for searching purposes because they are so common. Conversely, terms which occur in only a few documents have much greater discrimination power. This is exploited in the TF-IDF score by reweighting the TF score. The TF-IDF score of term $i$ in document $d$ is is defined by:
\[\text{TF-IDF}_d(i)= \text{TF}_d(i)\log{(|D|/|\lbrace d' : t(i) \in d' \rbrace|)}\]
The TF-IDF score of term $i$ drops according to the logarithm of the number of documents that contain term $j$. The TF-IDF score of terms which are contained in every single document is always equal to zero.

\subsubsection{Computing Similarity between Documents}\label{sec:computing_similarity_between_documents}

In VSM we need to measure the similarity between vectors. The similarity can expressed by the cosine similarity measure:

\[\text{cosim}(d,d') = \sum_{k \in \mathcal{T}}\frac{d(k)d'(k)}{|d||d'|}\]

It expresses the cosine angle between the vectors $d$ and $d'$. The cosine measure is equal to one for the paralell vectors and is equal to zero for perpendicular vectors.

We arrange the retrieved documents according to their similarity to each other. To this end, we store the similarity of each pair of documents in similarity matrices. The matrices correspond to the four different indexes: similarity according to persons, locations, organizations and general vocabulary. These matrices are not sparse and each cell is represented in the memory.   

\subsubsection{Keyword and Metadata Search}\label{sec:keyword_search}

For VSM the searching proceeds as follows. The search query is transformed into its vector representation in the same manner as it is done for the document. The cosine similarity between the query vector and the vector representation of each document is computed. The search results are ordered according to their similarity to the user query. 

Document retrieval in our model consists of two steps. In the first step, documents that do not satisfy the conditions concerning the type and date of the document, are filtered out. In the second step, documents are sorted in descending order according to their cosine measure similarity with the query. If the search is only on meta data, documents are not sorted. Only the subset of the best results of the size specified by the user proceeds to the next stage.