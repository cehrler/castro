We use the Vector Space Model (VSM) as our framework for representation of documents and queries. The VSM is an established approach in IR as it enables straightforward modeling of document-query similarity in order to determine relevance of documents to the query. More crucially, it enables us to model inter-document similarity which underlies the idea of linking similar and related documents. Both similarity notions can be modeled as proximity of vectors in the multidimensional vector space. Furthermore, the VSM is a convenient representation for implementation of clustering algorithms, which provide another tool for arranging the graph nodes in an informative manner.  

The VSM is a model for representing text documents or arbitrary data in general as vectors. In IR, the VSM is often used to represent both documents and queries as points in Euclidean space. In the VSM the value of each element of the vector corresponds to the importance of a term for the representation of the query or the document. The vectors in VSM are usually normalized in order to have the same length (e.g.\ unit length)--- Otherwise, the model would favor larger documents. The convenient and elegant algebraic formulation of the VSM is useful for has clear theoretical and practical advantages. 

Another advantage of the VSM is its flexibility as compared to more restricted models such as boolean models. In boolean models, documents that do not exactly match the query are rejected. However, in many real world scenarios, the situation is different: The relevance of the document for the user is often graded rather then binary. The VSM can model this relevance scale.


\subsubsection{Indexing}
\label{sec:indexing}
In IR, the index is a data structure which stores the vector representation of documents in a vector model. Given a set of documents $\mathcal{D}$ together with a set of terms $\mathcal{T}$ the index is a matrix $D \in \mathbb{R}^{|\mathcal{D}|\times |\mathcal{T}|}$ where $D_{d,i}$ corresponds to the importance of term $i$ in document $d$. In each column, the weights for one distinct term over all the documents is stored.

Index matrices are often very sparse: In our implementation, they are represented as a sparse matrix (e.g.\ linked lists)--- In the data structures, only coordinates of the non-zero entries are stored, all of the document vectors being normalized in advance. This allows the compute cosine similarities for documents and queries to be computed without being normalized first.

Our model contains several index matrices. We created an index matrix for each NE type and a general index for the non-NE terms (all terms occurring in the documents). This means that all the values corresponding to the search terms in the query remain the same in the vector representation of the query.

\subsubsection{Term Weighting}
\label{sec:term_weighting}
There are several measures that can be used to express the importance of the index term for the document, perhaps the most widely used being TF and TF-IDF.

TF is the frequency of a term in a given document. Hence, TF scores grow linearly with the number of occurrences of a term in the document. The TF score of term $i$ in document $d$ is defined as:
\[\text{TF}_d(i) = \frac{n_d(i)}{\sum_{i' \in \mathcal{T}}{n_d(i')}}\]
where $n_d(i)$ denotes the number of occurrences of term $i$ in document $d$ and $\mathcal{T}$ denotes the set of all terms. 

TF-IDF is the product of term frequency and the logarithm of inverse document frequency. This measure is based on the observation that terms which occur more commonly in documents overall are less useful than terms which occur less commonly in documents overall. The TF-IDF score of term $i$ in document $d$ is is defined as
\[\text{TF-IDF}_d(i)= \text{TF}_d(i)\log{\left(\frac{|D|}{|\lbrace d' : t(i) \in d' \rbrace|}\right)}\]
The TF-IDF score of term $i$ is proportional to the logarithm of the inverse fraction of documents that contain term $i$; The smaller the logarithm, the lower the TF-IDF score. Thus, the TF-IDF score of terms which are contained in every document always equals $0$.

\subsubsection{Computing Similarity between Documents}\label{sec:computing_similarity_between_documents}

In VSM, it is necessary to measure the similarity between vectors. This can expressed by the cosine similarity measure:
\[\text{cosim}(d,d') = \sum_{k \in \mathcal{T}}\frac{d(k)d'(k)}{|d||d'|}\]
It expresses the cosine angle between the vectors $d$ and $d'$. The cosine measure is equal to $1$ for the parallel vectors and is equal to $0$ for perpendicular vectors.

\note{ADD SOMETHING HERE!!!}

The retrieved documents were arranged according to their similarity to each other. To this end, the similarity of each pair of documents was stored in similarity matrices. The matrices correspond to the four different indices: similarity according to \meta{Persons}, \meta{Locations} and \meta{Organizations} and to general vocabulary. These matrices are not sparse (unlike the index matrices) and each entry is represented in the memory.   

\subsubsection{Keyword and Metadata Search}\label{sec:keyword_search}

Searching in VSMs is implemented by transforming a search query into its vector representation in the same fashion as a document is. The cosine similarity between the query vector and the vector representation of each document is computed. The search results are then ordered according to their similarity to the user query and are returned as output. 

Document retrieval using this model consists of two steps: Firstly, documents that do not satisfy the conditions concerning the type and date of the document are filtered out. Secondly, documents are sorted in descending order according to their cosine similarity to the query. However, if the search is solely on metadata, documents are not sorted. Only the subset of the best results of the size specified by the user proceeds to the next stage.

\note{ADD CONCLUSION: WHY is this useful? WHAT do we use this for? HOW does it help in the project? e.g.\ ``By precomputing a database of bla bla ba... aliasing can be performed in real time and I can get a good grade for this fucking course''}
