We use the Vector Space Model (VSM) as our framework for representation of documents and queries. The VSM is an established approach in IR as it enables straightforward modeling of document-query similarity in order to determine relevance of documents to the query. More crucially, it enables us to model inter-document similarity which underlies the idea of linking similar and related documents. Both similarity notions can be modeled as proximity of vectors in the multidimensional vector space. Furthermore, the VSM is a convenient representation for implementation of clustering algorithms, which provide another tool for arranging the graph nodes in an informative manner.  

The VSM is a model for representing arbitrary data as vectors in Euclidean space. In IR, this representation is used for both documents and queries. In the The value of each element of the vector corresponds to the importance of a term for the representation of the query or the document. In this basic formulation the model treats both documents and queries as ``bags of words'' ignoring any order or structure related information. The vectors are usually normalized in order to have the same length (e.g.\ unit length)--- Otherwise, the model would favor larger documents. 

The elegant algebraic formulation and interpretation of the VSM make this model attractive from for theoretical and practical perspectives.
Another advantage of the VSM is its flexibility as compared to more restricted models such as boolean models. In boolean models, documents that do not exactly match the query are rejected. However, in many real world scenarios, the situation is different: The relevance of the document for the user is often graded rather then binary. The VSM can model this relevance scale.


\subsubsection{Indexing}
\label{sec:indexing}
Representing the documents of our collection in vector space requires the construction of an index. Given a set of documents $\mathcal{D}$ together with a set of terms $\mathcal{T}$ the index is a matrix $D \in \mathbb{R}^{|\mathcal{D}|\times |\mathcal{T}|}$ where $D_{d,i}$ corresponds to the importance of term $i$ in document $d$. In each column, the weights for one distinct term over all the documents is stored.

Index matrices are often very sparse: In our implementation, they are represented as a sparse matrix (linked lists)--- In the data structures, only coordinates of the non-zero entries are stored, all of the document vectors being normalized in advance. This allows the compute cosine similarities for documents and queries to be computed without being normalized first.

Our model contains several index matrices. We created an index matrix for each NE type and a general index for the non-NE terms (all terms occurring in the documents). For the NE's indexes we used the output of the SNRE. For the non-NE terms (general vocabulary) we used all the vocabulary of the collection, without any filtering. Indexes are also constructed on-the-fly for queries, using the query terms.

\subsubsection{Term Weighting}
\label{sec:term_weighting}
There are several measures that can be used to express the importance of the index term for the document, perhaps the most widely used being TF and TF-IDF.

TF is the frequency of a term in a given document. Hence, TF scores grow linearly with the number of occurrences of a term in the document. The TF score of term $i$ in document $d$ is defined as:
\[\text{TF}_d(i) = \frac{n_d(i)}{\sum_{i' \in \mathcal{T}}{n_d(i')}}\]
where $n_d(i)$ denotes the number of occurrences of term $i$ in document $d$ and $\mathcal{T}$ denotes the set of all terms. 

TF-IDF is the product of term frequency and the logarithm of inverse document frequency. This measure is based on the observation that terms which occur more commonly in documents overall are less useful than terms which occur less commonly in documents overall. The TF-IDF score of term $i$ in document $d$ is is defined as
\[\text{TF-IDF}_d(i)= \text{TF}_d(i)\log{\left(\frac{|D|}{|\lbrace d' : t(i) \in d' \rbrace|}\right)}\]
The TF-IDF score of term $i$ is proportional to the logarithm of the inverse fraction of documents that contain term $i$; The smaller the logarithm, the lower the TF-IDF score. Thus, the TF-IDF score of terms which are contained in every document always equals $0$.

The difference between these two weighting schemes can have a considerable impact on the ranking of the query results as well as on the similarity measurements between the documents. Using only TF may result in an unjustified high similarity between a pair of documents due to appearances of non-discriminative terms that are frequent in every document. For instance, two completely unrelated documents can have relatively high similarity measure simply since they share NE's such as \lingform{Fidel Castro} and \lingform{Cuba} or non-NE's such functional words. TF-IDF accounts for this problem by assigning low weights for non-discriminative terms. A similar problem can happen with query-document similarity. With respect to this issue, it seems that TF/IDF is a more suitable vector representation for our similarity measurements. However, since it is possible to think of scenarios where the TF could be useful, we enable the user to switch between the two, setting TF/IDF as default. 


