\note{ADD INTRODUCTION: WHAT do we do with the data in the databases? WHY do we do it? WHAT does this enable us to do? (i.e.\ later)}

The Vector Space Model (VSM) is a model for representing text documents or arbitrary data in general as vectors. In IR, the VSM is often used to represent both documents and queries as points in Euclidean space. In the VSM the value of each element of the vector corresponds to the importance of a term for the representation of the query or the document. The main advantage of the VSM is that it has a convenient algebraic formulation. It is also more flexible than boolean models. In boolean models, documents that do not exactly match the query are rejected. However, in many real word scenarios, the situation is different: The relevance of the document for the user is often graded rather then binary. The VSM can model this relevance scale.

The vectors in VSM are usually normalized in order to have the same length (e.g.\ unit length)--- Otherwise, the model would favor larger documents.

\subsubsection{Indexing}
\label{sec:indexing}
In IR, the index is a data structure which stores the vector representation of documents in a vector model. Given a set of documents $\mathcal{D}$ together with a set of terms $\mathcal{T}$ the index is a matrix $D \in \mathbb{R}^{|\mathcal{D}|\times |\mathcal{T}|}$ where $D_{d,i}$ corresponds to the importance of term $i$ in document $d$. In each column, the weights for one distinct term over all the documents is stored.

Index matrices are often very sparse: In our implementation, they are represented as a sparse matrix (e.g.\ linked lists)--- In the data structures, only coordinates of the non-zero entries are stored, all of the document vectors being normalized in advance. This allows the compute cosine similarities for documents and queries to be computed without being normalized first.

Our model contains several index matrices. We created an index matrix for each NE type and a general index for the non-NE terms (all terms occurring in the documents). This means that all the values corresponding to the search terms in the query remain the same in the vector representation of the query.

\subsubsection{Term Weighting}
\label{sec:term_weighting}
There are several measures that can be used to express the importance of the index term for the document, perhaps the most widely used being TF and TF-IDF.

TF is the frequency of a term in a given document. Hence, TF scores grow linearly with the number of occurrences of a term in the document. The TF score of term $i$ in document $d$ is defined as:
\[\text{TF}_d(i) = \frac{n_d(i)}{\sum_{i' \in \mathcal{T}}{n_d(i')}}\]
where $n_d(i)$ denotes the number of occurrences of term $i$ in document $d$ and $\mathcal{T}$ denotes the set of all terms. 

TF-IDF is the product of term frequency and the logarithm of inverse document frequency. This measure is based on the observation that terms which occur more commonly in documents overall are less useful than terms which occur less commonly in documents overall. The TF-IDF score of term $i$ in document $d$ is is defined as
\[\text{TF-IDF}_d(i)= \text{TF}_d(i)\log{\left(\frac{|D|}{|\lbrace d' : t(i) \in d' \rbrace|}\right)}\]
The TF-IDF score of term $i$ is proportional to the logarithm of the inverse fraction of documents that contain term $i$; The smaller the logarithm, the lower the TF-IDF score. Thus, the TF-IDF score of terms which are contained in every document always equals $0$.

\subsubsection{Computing Similarity between Documents}\label{sec:computing_similarity_between_documents}

For the goal of our project, it is necessary to measure the similarity between the documents. According to similarities of the documents, edges of the resulting graph are constructed. The crucial and not easy to be answered question is, how does the similar documents look like. Our project aims at making navigation throw the collection of historical document. Historicians usually aim at some specific historical event or historical person. It follows that it is convenient to suppose that two documents, that touches the same or similar historical topic, are similar. The historical event can be usually specified by the persons, locations and organizations involved in it. Our working hypothese is that the similarity between documents, we are interrested in, can be expressed as the similarity on the named entities. As we represent documents in VSM as the vectors of named entities, we can directly apply standard vector space measures for expressing similarities between vectors. 

The most common measure is the cosine similarity measure.
\[\text{cosim}(d,d') = \sum_{k \in \mathcal{T}}\frac{d(k)d'(k)}{|d||d'|}\]
It expresses the cosine angle between the vectors $d$ and $d'$. The cosine measure is equal to $1$ for the parallel vectors and is equal to $0$ for perpendicular vectors.

We also used another similarity measure which we denoted as "Manhattan similarity measure". The reason is that the Manhattan similarity measure between two vectors can be described using the manhattan distance between zero vector and the vector whose value for each column is equal to the minimum value of those vectors for that column.

\[\text{manhattan}(d,d') = \sum_{k \in \mathcal{T}}min[d(k), d'(k)]\]


\note{Michal: I added some stuff here. You are welcomed to fix it and complete it. ADD SOMETHING HERE!!!}

The retrieved documents were arranged according to their similarity to each other. To this end, the similarity of each pair of documents was stored in similarity matrices. The matrices correspond to the four different indices: similarity according to \meta{Persons}, \meta{Locations} and \meta{Organizations} and to general vocabulary. These matrices are not sparse (unlike the index matrices) and each entry is represented in the memory.   

\subsubsection{Keyword and Metadata Search}\label{sec:keyword_search}

Searching in VSMs is implemented by transforming a search query into its vector representation in the same fashion as a document is. The cosine similarity between the query vector and the vector representation of each document is computed. The search results are then ordered according to their similarity to the user query and are returned as output. 

Document retrieval using this model consists of two steps: Firstly, documents that do not satisfy the conditions concerning the type and date of the document are filtered out. Secondly, documents are sorted in descending order according to their cosine similarity to the query. However, if the search is solely on metadata, documents are not sorted. Only the subset of the best results of the size specified by the user proceeds to the next stage.

\note{ADD CONCLUSION: WHY is this useful? WHAT do we use this for? HOW does it help in the project? e.g.\ ``By precomputing a database of bla bla ba... aliasing can be performed in real time and I can get a good grade for this fucking course''}
